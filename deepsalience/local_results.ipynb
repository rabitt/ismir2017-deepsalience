{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Reshape, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15400992706783082501\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import experiment\n",
    "import core\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = '/Users/rabitt/Desktop/multif0_models/'\n",
    "model_name = 'multif0_exper10_batchin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import glob\n",
    "import librosa\n",
    "import mir_eval\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import scipy\n",
    "\n",
    "import compute_training_data as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (None, None, 6)\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "y0 = BatchNormalization()(inputs)\n",
    "y1 = Conv2D(256, (5, 5), padding='same', activation='relu', name='bendy1')(y0)\n",
    "y1a = BatchNormalization()(y1)\n",
    "y2 = Conv2D(16, (70, 3), padding='same', activation='relu', name='harmonics')(y1a)\n",
    "y2a = BatchNormalization()(y2)\n",
    "y3 = Conv2D(128, (3, 3), padding='same', activation='relu', name='smoothy1')(y2a)\n",
    "y3a = BatchNormalization()(y3)\n",
    "y4 = Conv2D(8, (360, 1), padding='same', activation='relu', name='distribution')(y3a)\n",
    "y4a = BatchNormalization()(y4)\n",
    "y5 = Conv2D(1, (1, 1), padding='same', activation='sigmoid', name='squishy')(y4a)\n",
    "predictions = Lambda(lambda x: K.squeeze(x, axis=3))(y5)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_path, \"{}.pkl\".format(model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_on_test_set(model, thresh=0.3, save_path='/Users/rabitt/Desktop/'):\n",
    "    \"\"\"score a model on all files in a named test set\n",
    "    \"\"\"\n",
    "\n",
    "    # get files for this test set\n",
    "    test_set_path = os.path.join(\"../comparisons/mdb_test/\")\n",
    "    test_npy_files = glob.glob(os.path.join(test_set_path, '*.npy'))\n",
    "\n",
    "    all_scores = []\n",
    "    for npy_file in sorted(test_npy_files):\n",
    "        # get input npy file and ground truth label pair\n",
    "        file_keys = os.path.basename(npy_file).replace('-', '_').split('_')[:2]\n",
    "        label_file = glob.glob(\n",
    "            os.path.join(\n",
    "                test_set_path,\n",
    "                \"{}*{}*.txt\".format(file_keys[0], file_keys[1]))\n",
    "        )[0]\n",
    "\n",
    "        # generate prediction on numpy file\n",
    "        predicted_output, input_hcqt = \\\n",
    "            evaluate.get_single_test_prediction(npy_file, model)\n",
    "\n",
    "        # save plot for first example\n",
    "        if len(all_scores) == 0:\n",
    "            plot_save_path = os.path.join(\n",
    "                save_path,\n",
    "                \"{}_{}_plot_output.pdf\".format(file_keys[0], file_keys[1])\n",
    "            )\n",
    "\n",
    "            plt.figure(figsize=(15, 15))\n",
    "\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.imshow(input_hcqt[0, :, :, 1], origin='lower')\n",
    "            plt.axis('auto')\n",
    "            plt.colorbar()\n",
    "\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.imshow(predicted_output, origin='lower')\n",
    "            plt.axis('auto')\n",
    "            plt.colorbar()\n",
    "\n",
    "            plt.savefig(plot_save_path, format='pdf')\n",
    "            plt.close()\n",
    "\n",
    "        # save prediction\n",
    "        np.save(\n",
    "            os.path.join(\n",
    "                save_path,\n",
    "                \"{}_{}_prediction.npy\".format(file_keys[0], file_keys[1])\n",
    "            ),\n",
    "            predicted_output.astype(np.float32)\n",
    "        )\n",
    "\n",
    "        # get multif0 output from prediction\n",
    "        est_times, est_freqs = evaluate.pitch_activations_to_mf0(predicted_output, thresh)\n",
    "\n",
    "        # save multif0 output\n",
    "        evaluate.save_multif0_output(\n",
    "            est_times, est_freqs,\n",
    "            os.path.join(\n",
    "                save_path,\n",
    "                \"{}_{}_prediction.txt\".format(file_keys[0], file_keys[1])\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # load ground truth labels\n",
    "        ref_times, ref_freqs = \\\n",
    "            mir_eval.io.load_ragged_time_series(label_file)\n",
    "\n",
    "        # get multif0 metrics and append\n",
    "        scores = mir_eval.multipitch.evaluate(\n",
    "            ref_times, ref_freqs, est_times, est_freqs)\n",
    "        scores['track'] = '_'.join(file_keys)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "    # save scores to data frame\n",
    "    scores_path = os.path.join(\n",
    "        save_path, '{}_all_scores.csv'.format(test_set_name)\n",
    "    )\n",
    "    score_summary_path = os.path.join(\n",
    "        save_path, \"{}_score_summary.csv\".format(test_set_name)\n",
    "    )\n",
    "    df = pandas.DataFrame(all_scores)\n",
    "    df.to_csv(scores_path)\n",
    "    df.describe().to_csv(score_summary_path)\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4503c8f4e637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore_on_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a83f4d2e057e>\u001b[0m in \u001b[0;36mscore_on_test_set\u001b[0;34m(model, thresh, save_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# generate prediction on numpy file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicted_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_hcqt\u001b[0m \u001b[0;34m=\u001b[0m             \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_single_test_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# save plot for first example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rabitt/Dropbox/MARL/repos/multif0/deepsalience/evaluate.py\u001b[0m in \u001b[0;36mget_single_test_prediction\u001b[0;34m(npy_file, model)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \"\"\"Generate output from a model given an input numpy file\n\u001b[1;32m    270\u001b[0m     \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0minput_hcqt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpy_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mn_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_hcqt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "score_on_test_set(model, thresh=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
